{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisaespinoza/CSCI167-Notebook12/blob/main/Copy_of_12_2_Multihead_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook 12.2: Multihead Self-Attention**\n",
        "\n",
        "This notebook builds a multihead self-attention mechanism as in figure 12.6\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
        "\n"
      ],
      "metadata": {
        "id": "t9vk9Elugvmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OLComQyvCIJ7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The multihead self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
        "\n"
      ],
      "metadata": {
        "id": "9OJkkoNqCVK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(3)\n",
        "# Number of inputs\n",
        "N = 6\n",
        "# Number of dimensions of each input\n",
        "D = 8\n",
        "# Create an empty list\n",
        "X = np.random.normal(size=(D,N))\n",
        "# Print X\n",
        "print(X)"
      ],
      "metadata": {
        "id": "oAygJwLiCSri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1e6928-864c-4c5d-864e-10bdf58ccdbe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.789  0.437  0.096 -1.863 -0.277 -0.355]\n",
            " [-0.083 -0.627 -0.044 -0.477 -1.314  0.885]\n",
            " [ 0.881  1.71   0.05  -0.405 -0.545 -1.546]\n",
            " [ 0.982 -1.101 -1.185 -0.206  1.486  0.237]\n",
            " [-1.024 -0.713  0.625 -0.161 -0.769 -0.23 ]\n",
            " [ 0.745  1.976 -1.244 -0.626 -0.804 -2.419]\n",
            " [-0.924 -1.024  1.124 -0.132 -1.623  0.647]\n",
            " [-0.356 -1.743 -0.597 -0.589 -0.874  0.03 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use two heads.  We'll need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4).  We'll use two heads, and (as in the figure), we'll make the queries keys and values of size D/H"
      ],
      "metadata": {
        "id": "W2iHFbtKMaDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of heads\n",
        "H = 2\n",
        "# QDV dimension\n",
        "H_D = int(D/H)\n",
        "\n",
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Choose random values for the parameters for the first head\n",
        "omega_q1 = np.random.normal(size=(H_D,D))\n",
        "omega_k1 = np.random.normal(size=(H_D,D))\n",
        "omega_v1 = np.random.normal(size=(H_D,D))\n",
        "beta_q1 = np.random.normal(size=(H_D,1))\n",
        "beta_k1 = np.random.normal(size=(H_D,1))\n",
        "beta_v1 = np.random.normal(size=(H_D,1))\n",
        "\n",
        "# Choose random values for the parameters for the second head\n",
        "omega_q2 = np.random.normal(size=(H_D,D))\n",
        "omega_k2 = np.random.normal(size=(H_D,D))\n",
        "omega_v2 = np.random.normal(size=(H_D,D))\n",
        "beta_q2 = np.random.normal(size=(H_D,1))\n",
        "beta_k2 = np.random.normal(size=(H_D,1))\n",
        "beta_v2 = np.random.normal(size=(H_D,1))\n",
        "\n",
        "# Choose random values for the parameters\n",
        "omega_c = np.random.normal(size=(D,D))"
      ],
      "metadata": {
        "id": "79TSK7oLMobe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute the multiscale self-attention"
      ],
      "metadata": {
        "id": "VxaKQtP3Ng6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define softmax operation that works independently on each column\n",
        "def softmax_cols(data_in):\n",
        "  # Exponentiate all of the values\n",
        "  exp_values = np.exp(data_in) ;\n",
        "  # Sum over columns\n",
        "  denom = np.sum(exp_values, axis = 0);\n",
        "  # Compute softmax (numpy broadcasts denominator to all rows automatically)\n",
        "  softmax = exp_values / denom\n",
        "  # return the answer\n",
        "  return softmax"
      ],
      "metadata": {
        "id": "obaQBdUAMXXv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Now let's compute self attention in matrix form\n",
        "def multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c):\n",
        "\n",
        "  # Compute queries, keys, and values for head 1\n",
        "  Q1 = omega_q1 @ X + beta_q1\n",
        "  K1 = omega_k1 @ X + beta_k1\n",
        "  V1 = omega_v1 @ X + beta_v1\n",
        "\n",
        "  print(\"Q1:\")\n",
        "  print(Q1)\n",
        "  print(\"K1:\")\n",
        "  print(K1)\n",
        "  print(\"V1:\")\n",
        "  print(V1)\n",
        "\n",
        "  # Compute attention scores for head 1\n",
        "  attention_scores1 = Q1.T @ K1\n",
        "  print(\"attention_scores1:\")\n",
        "  print(attention_scores1)\n",
        "\n",
        "\n",
        "  # Scale attention scores for head 1\n",
        "  scaled_attention_scores1 = attention_scores1 / np.sqrt(H_D)\n",
        "  print(\"scaled_attention_scores1:\")\n",
        "  print(scaled_attention_scores1)\n",
        "\n",
        "\n",
        "  # Apply softmax to get attention weights for head 1\n",
        "  attention_weights1 = softmax_cols(scaled_attention_scores1)\n",
        "  print(\"attention_weights1:\")\n",
        "  print(attention_weights1)\n",
        "\n",
        "\n",
        "  # Compute the weighted sum of values for head 1\n",
        "  head1_output = V1 @ attention_weights1\n",
        "  print(\"head1_output:\")\n",
        "  print(head1_output)\n",
        "\n",
        "\n",
        "  # Compute queries, keys, and values for head 2\n",
        "  Q2 = omega_q2 @ X + beta_q2\n",
        "  K2 = omega_k2 @ X + beta_k2\n",
        "  V2 = omega_v2 @ X + beta_v2\n",
        "\n",
        "  print(\"Q2:\")\n",
        "  print(Q2)\n",
        "  print(\"K2:\")\n",
        "  print(K2)\n",
        "  print(\"V2:\")\n",
        "  print(V2)\n",
        "\n",
        "  # Compute attention scores for head 2\n",
        "  attention_scores2 = Q2.T @ K2\n",
        "  print(\"attention_scores2:\")\n",
        "  print(attention_scores2)\n",
        "\n",
        "\n",
        "  # Scale attention scores for head 2\n",
        "  scaled_attention_scores2 = attention_scores2 / np.sqrt(H_D)\n",
        "  print(\"scaled_attention_scores2:\")\n",
        "  print(scaled_attention_scores2)\n",
        "\n",
        "\n",
        "  # Apply softmax to get attention weights for head 2\n",
        "  attention_weights2 = softmax_cols(scaled_attention_scores2)\n",
        "  print(\"attention_weights2:\")\n",
        "  print(attention_weights2)\n",
        "\n",
        "\n",
        "  # Compute the weighted sum of values for head 2\n",
        "  head2_output = V2 @ attention_weights2\n",
        "  print(\"head2_output:\")\n",
        "  print(head2_output)\n",
        "\n",
        "\n",
        "  # Concatenate the outputs of the two heads\n",
        "  concatenated_heads = np.vstack((head1_output, head2_output))\n",
        "  print(\"concatenated_heads:\")\n",
        "  print(concatenated_heads)\n",
        "\n",
        "\n",
        "  # Apply the final linear transformation\n",
        "  X_prime = omega_c @ concatenated_heads\n",
        "  print(\"X_prime before return:\")\n",
        "  print(X_prime)\n",
        "\n",
        "\n",
        "  return X_prime"
      ],
      "metadata": {
        "id": "gb2WvQ3SiH8r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comment in the code block below suggests looking at scaling if your values don't match. That said , it didn't neccessarily indicate a scaling method. I added print statements to track the value changes but after investigating at length, I've abandoned tracking down the issue."
      ],
      "metadata": {
        "id": "C2i7Da5i3c9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the self attention mechanism\n",
        "X_prime = multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c)\n",
        "\n",
        "# Print out the results\n",
        "np.set_printoptions(precision=3)\n",
        "print(\"Your answer:\")\n",
        "print(X_prime)\n",
        "\n",
        "print(\"True values:\")\n",
        "print(\"[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\")\n",
        "print(\" [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\")\n",
        "print(\" [  5.479   1.115   9.244   0.453   5.656   7.089]\")\n",
        "print(\" [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\")\n",
        "print(\" [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\")\n",
        "print(\" [  3.548  10.036  -2.244   1.604  12.113  -2.557]\")\n",
        "print(\" [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\")\n",
        "print(\" [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\")\n",
        "\n",
        "# If your answers don't match, then make sure that you are doing the scaling, and make sure the scaling value is correct"
      ],
      "metadata": {
        "id": "MUOJbgJskUpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2626a6e3-97fd-4363-b8ee-aa3536dc935f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1:\n",
            "[[ 2.733 -4.236  1.098 -4.049 -0.268  1.3  ]\n",
            " [ 1.905 -1.21   0.666  0.972  1.663  2.135]\n",
            " [ 4.82   5.904  0.313 -2.187 -0.775 -1.728]\n",
            " [ 3.686  3.582 -0.012 -4.523 -1.561 -5.601]]\n",
            "K1:\n",
            "[[ 0.408  4.393  0.7    3.87   4.135 -2.43 ]\n",
            " [-1.617 -6.555 -5.429  1.637  6.737  1.258]\n",
            " [-5.032 -6.485 -1.058  2.694  1.533  3.558]\n",
            " [ 1.935  3.734  1.504  2.489  3.995  2.556]]\n",
            "V1:\n",
            "[[-1.557 -4.362 -1.658 -0.806  0.242  1.597]\n",
            " [ 4.494  4.355  2.731  0.898  3.604  1.064]\n",
            " [-4.807  2.793  1.282 -0.021 -8.235 -4.459]\n",
            " [-1.971 -3.726 -3.472 -0.452 -1.94  -0.681]]\n",
            "attention_scores1:\n",
            "[[-19.088 -17.975  -7.989  35.855  46.25   22.328]\n",
            " [-22.549 -35.59    2.744   6.449  -2.305  38.935]\n",
            " [ -2.227  -1.616  -3.197   6.153   9.458  -0.749]\n",
            " [ -0.968 -26.86  -12.596 -31.228 -31.617  -8.282]\n",
            " [ -1.919 -12.882 -10.743  -4.292   2.665  -4.004]\n",
            " [ -5.063 -17.992 -17.274 -10.07   -5.267 -20.935]]\n",
            "scaled_attention_scores1:\n",
            "[[ -9.544  -8.988  -3.995  17.928  23.125  11.164]\n",
            " [-11.274 -17.795   1.372   3.224  -1.152  19.467]\n",
            " [ -1.114  -0.808  -1.598   3.076   4.729  -0.375]\n",
            " [ -0.484 -13.43   -6.298 -15.614 -15.808  -4.141]\n",
            " [ -0.959  -6.441  -5.372  -2.146   1.333  -2.002]\n",
            " [ -2.532  -8.996  -8.637  -5.035  -2.633 -10.467]]\n",
            "attention_weights1:\n",
            "[[5.090e-05 2.791e-04 4.415e-03 1.000e+00 1.000e+00 2.476e-04]\n",
            " [9.019e-06 4.176e-08 9.455e-01 4.116e-07 2.860e-11 9.998e-01]\n",
            " [2.333e-01 9.959e-01 4.849e-02 3.550e-07 1.025e-08 2.413e-09]\n",
            " [4.380e-01 3.284e-06 4.413e-04 2.711e-15 1.234e-17 5.584e-11]\n",
            " [2.722e-01 3.563e-03 1.114e-03 1.915e-09 3.433e-10 4.740e-10]\n",
            " [5.650e-02 2.768e-04 4.256e-05 1.065e-10 6.504e-12 9.985e-14]]\n",
            "head1_output:\n",
            "[[-0.584 -1.65  -4.212 -1.557 -1.557 -4.361]\n",
            " [ 2.072  2.734  4.275  4.494  4.494  4.355]\n",
            " [-2.204  1.245  2.672 -4.807 -4.807  2.791]\n",
            " [-1.574 -3.465 -3.703 -1.971 -1.971 -3.726]]\n",
            "Q2:\n",
            "[[ 8.76   6.01  -3.914 -6.087 -2.65  -5.978]\n",
            " [-0.415 -6.13  -1.434 -3.938 -1.689 -0.016]\n",
            " [ 0.313 -2.84  -3.572  0.558  3.285 -0.422]\n",
            " [-2.565 -2.207  0.079  1.251  0.796  1.268]]\n",
            "K2:\n",
            "[[-2.944 -1.906 -2.005  1.14  -1.117 -2.3  ]\n",
            " [-0.317 -2.231 -1.391  3.29   5.433  2.559]\n",
            " [ 2.613  3.643 -0.169 -1.425 -0.794 -2.711]\n",
            " [-6.288 -2.475  3.136  2.222 -5.144  2.245]]\n",
            "V2:\n",
            "[[ 2.854 -1.304 -0.129  0.389  4.877  3.226]\n",
            " [-4.569 -0.983  3.274  2.585  1.039  1.931]\n",
            " [-4.524 -7.256  0.655  0.936 -1.395  5.914]\n",
            " [ 3.072 -0.841 -0.935 -3.713 -2.781 -1.59 ]]\n",
            "attention_scores2:\n",
            "[[ -8.711  -8.28  -25.08    2.474   0.906 -27.815]\n",
            " [ -9.294  -2.66   -9.963 -14.176 -26.411 -26.766]\n",
            " [  2.149  -2.547  10.695  -3.915  -0.991  15.192]\n",
            " [ 12.761  19.326  21.509 -17.913 -21.477   5.218]\n",
            " [ 11.913  18.816   9.601 -11.492 -12.922  -5.345]\n",
            " [  8.531   6.756  16.055  -3.45    0.401  17.696]]\n",
            "scaled_attention_scores2:\n",
            "[[ -4.356  -4.14  -12.54    1.237   0.453 -13.907]\n",
            " [ -4.647  -1.33   -4.982  -7.088 -13.206 -13.383]\n",
            " [  1.075  -1.273   5.348  -1.957  -0.495   7.596]\n",
            " [  6.38    9.663  10.755  -8.957 -10.738   2.609]\n",
            " [  5.957   9.408   4.801  -5.746  -6.461  -2.673]\n",
            " [  4.265   3.378   8.027  -1.725   0.2     8.848]]\n",
            "attention_weights2:\n",
            "[[1.222e-05 5.698e-07 7.128e-11 9.141e-01 4.619e-01 1.018e-10]\n",
            " [9.127e-06 9.463e-06 1.366e-07 2.216e-04 5.402e-07 1.719e-10]\n",
            " [2.788e-03 1.002e-05 4.182e-03 3.748e-02 1.789e-01 2.220e-01]\n",
            " [5.617e-01 5.629e-01 9.324e-01 3.420e-05 6.370e-06 1.515e-03]\n",
            " [3.677e-01 4.360e-01 2.420e-03 8.479e-04 4.591e-04 7.705e-06]\n",
            " [6.776e-02 1.049e-03 6.098e-02 4.727e-02 3.588e-01 7.765e-01]]\n",
            "head2_output:\n",
            "[[ 2.23   2.349  0.571  2.761  2.455  2.477]\n",
            " [ 1.974  1.91   2.545 -3.962 -0.831  2.23 ]\n",
            " [ 0.415 -0.076  1.232 -3.835  0.148  4.738]\n",
            " [-3.219 -3.304 -3.57   2.695  0.68  -1.448]]\n",
            "concatenated_heads:\n",
            "[[-0.584 -1.65  -4.212 -1.557 -1.557 -4.361]\n",
            " [ 2.072  2.734  4.275  4.494  4.494  4.355]\n",
            " [-2.204  1.245  2.672 -4.807 -4.807  2.791]\n",
            " [-1.574 -3.465 -3.703 -1.971 -1.971 -3.726]\n",
            " [ 2.23   2.349  0.571  2.761  2.455  2.477]\n",
            " [ 1.974  1.91   2.545 -3.962 -0.831  2.23 ]\n",
            " [ 0.415 -0.076  1.232 -3.835  0.148  4.738]\n",
            " [-3.219 -3.304 -3.57   2.695  0.68  -1.448]]\n",
            "X_prime before return:\n",
            "[[ -8.674  -0.438   4.169  -7.239 -10.847   2.155]\n",
            " [ -0.485   1.986   5.378   4.897  -0.342  -1.476]\n",
            " [ -0.258  -4.826  -8.222  19.25   12.646  -1.832]\n",
            " [ -6.684  -6.75   -5.141   4.712   1.972   1.183]\n",
            " [ -5.068  -2.648  -1.678  -5.815  -5.505   2.671]\n",
            " [ -3.645 -10.214 -11.863   2.19   -1.302 -16.376]\n",
            " [  8.816  12.644  20.494  -9.078  -1.5    15.751]\n",
            " [ -4.896  -7.349 -16.751  17.79    5.026 -18.842]]\n",
            "Your answer:\n",
            "[[ -8.674  -0.438   4.169  -7.239 -10.847   2.155]\n",
            " [ -0.485   1.986   5.378   4.897  -0.342  -1.476]\n",
            " [ -0.258  -4.826  -8.222  19.25   12.646  -1.832]\n",
            " [ -6.684  -6.75   -5.141   4.712   1.972   1.183]\n",
            " [ -5.068  -2.648  -1.678  -5.815  -5.505   2.671]\n",
            " [ -3.645 -10.214 -11.863   2.19   -1.302 -16.376]\n",
            " [  8.816  12.644  20.494  -9.078  -1.5    15.751]\n",
            " [ -4.896  -7.349 -16.751  17.79    5.026 -18.842]]\n",
            "True values:\n",
            "[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\n",
            " [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\n",
            " [  5.479   1.115   9.244   0.453   5.656   7.089]\n",
            " [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\n",
            " [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\n",
            " [  3.548  10.036  -2.244   1.604  12.113  -2.557]\n",
            " [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\n",
            " [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\n"
          ]
        }
      ]
    }
  ]
}